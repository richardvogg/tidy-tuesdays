library(ggplot2)
library(dplyr)
library(stringr)
library(stringdist)
tuesdata <- tidytuesdayR::tt_load(2020, week = 35)
chopped <- tuesdata$chopped
#Make judges columns tidy - for our analysis we will ignore order of judges
judge_df <- chopped %>% select(season:judge3) %>%
tidyr::pivot_longer(cols=starts_with("judge"),names_to="judge_nr",values_to="judge")
#How often did each judge appear?
rank_before <- judge_df %>%
dplyr::count(judge,sort=T) %>%
tibble() %>%
mutate(txt=ifelse(is.na(txt),"None",txt))
#How often did each judge appear?
rank_before <- judge_df %>%
dplyr::count(judge,sort=T) %>%
tibble() %>%
mutate(txt=ifelse(is.na(judge),"None",judge))
#The core of fuzzy duplicates - Comparing each judge name with all the judge names before
#We store the closest name and the distance between both
out <- sapply(seq_along(test$txt)[-1],function(i) {
dist2 <- stringdist(test$txt[i],test$txt[1:i-1],method='jw',p=0.1)
best_fit <- which.min(dist2)
similarity2 <- min(dist2)
return(c(similarity2,best_fit))
}) %>%
as.data.frame(t(out)) %>%
add_row(V1=1,V2=1,.before = 1) %>%
cbind(test) %>%
dplyr::rename(distance=V1,best_fit=V2)
names <- rank_before$txt
#The core of fuzzy duplicates - Comparing each judge name with all the judge names before
#We store the closest name and the distance between both
out <- sapply(seq_along(names)[-1],function(i) {
dist2 <- stringdist(names[i],names[1:i-1],method='jw',p=0.1)
best_fit <- which.min(dist2)
similarity2 <- min(dist2)
return(c(similarity2,best_fit))
}) %>%
as.data.frame(t(out)) %>%
add_row(V1=1,V2=1,.before = 1) %>%
cbind(test) %>%
dplyr::rename(distance=V1,best_fit=V2)
#The core of fuzzy duplicates - Comparing each judge name with all the judge names before
#We store the closest name and the distance between both
out <- sapply(seq_along(names)[-1],function(i) {
dist2 <- stringdist(names[i],names[1:i-1],method='jw',p=0.1)
best_fit <- which.min(dist2)
similarity2 <- min(dist2)
return(c(similarity2,best_fit))
}) %>%
t() %>%
as.data.frame() %>%
add_row(V1=1,V2=1,.before = 1) %>%
cbind(test) %>%
dplyr::rename(distance=V1,best_fit=V2)
#The core of fuzzy duplicates - Comparing each judge name with all the judge names before
#We store the closest name and the distance between both
out <- sapply(seq_along(names)[-1],function(i) {
dist2 <- stringdist(names[i],names[1:i-1],method='jw',p=0.1)
best_fit <- which.min(dist2)
similarity2 <- min(dist2)
return(c(similarity2,best_fit))
}) %>%
t() %>%
as.data.frame() %>%
add_row(V1=1,V2=1,.before = 1) %>%
cbind(rank_before) %>%
dplyr::rename(distance=V1,best_fit=V2)
View(out)
#How often did each judge appear?
rank_before <- judge_df %>%
dplyr::count(judge,sort=T) %>%
tibble() %>%
mutate(judge=ifelse(is.na(judge),"None",judge))
names <- rank_before$judge
#The core of fuzzy duplicates - Comparing each judge name with all the judge names before
#We store the closest name and the distance between both
out <- sapply(seq_along(names)[-1],function(i) {
dist2 <- stringdist(names[i],names[1:i-1],method='jw',p=0.1)
best_fit <- which.min(dist2)
similarity2 <- min(dist2)
return(c(similarity2,best_fit))
}) %>%
t() %>%
as.data.frame() %>%
add_row(V1=1,V2=1,.before = 1) %>%
cbind(rank_before) %>%
dplyr::rename(distance=V1,best_fit=V2)
View(out)
#The core of fuzzy duplicates - Comparing each judge name with all the judge names before
#We store the closest name and the distance between both
out <- sapply(seq_along(names)[-1],function(i) {
dist2 <- stringdist(names[i],names[1:i-1],method='jw',p=0.1)
best_fit <- which.min(dist2)
similarity2 <- min(dist2)
return(c(similarity2,best_fit))
}) %>%
t() %>%
as.data.frame() %>%
add_row(V1=1,V2=1,.before = 1) %>%
cbind(rank_before) %>%
dplyr::rename(distance=V1,best_fit=V2) %>%
mutate(replacement=judge[best_fit])
View(out)
#Convert into a dataframe and find a decision rule for replacement
#Just keep the ones that change
out <- out %>%
mutate(replacement=ifelse(distance<0.06,replacement,judge)) %>%
filter(replacement!=judge)
View(out)
#Replace judge names
judge_df2 <- judge_df %>%
mutate(judge=plyr::mapvalues(judge,from=out$txt,to=out$replacement))
#Replace judge names
judge_df2 <- judge_df %>%
mutate(judge=plyr::mapvalues(judge,from=out$judge,to=out$replacement))
#How often did each judge REALLY appear?
rank_after <- judge_df2 %>%
dplyr::count(judge,sort=T) %>%
tibble() %>%
mutate(judge=ifelse(is.na(judge),"None",judge))
View(rank_after)
View(rank_before)
#Mini Example
test <- tibble(txt=c("hello","hallo","telefon","telephone"))
#Mini Example
test <- c("hello","hallo","telefon","telephone")
vec <- test
wordcount <- vec %>%
tibble(txt=vec)
wordcount <- vec %>%
tibble(txt=vec) %>%
dplyr::count(txt,sort=T)
View(wordcount)
#Mini Example
test <- c("hello","hallo","telefon","hello","telephone","telephone")
wordcount <- vec %>%
tibble(txt=vec) %>%
dplyr::count(txt,sort=T)
View(wordcount)
wordcount <- vec %>%
tibble(txt=vec) %>%
dplyr::count(txt,sort=T)
words <- wordcount$txt
View(wordcount)
vec <- test
wordcount <- vec %>%
tibble(txt=vec) %>%
dplyr::count(txt,sort=T)
words <- wordcount$txt
out <- sapply(seq_along(words)[-1],function(i) {
dist2 <- stringdist(words[i],words[1:i-1],method='jw',p=0.1)
best_fit <- which.min(dist2)
similarity2 <- min(dist2)
return(c(similarity2,best_fit))
}) %>%
t() %>%
as.data.frame() %>%
add_row(V1=1,V2=1,.before = 1) %>%
cbind(wordcount) %>%
dplyr::rename(distance=V1,best_fit=V2) %>%
mutate(replacement=txt[best_fit])
View(out)
remove_duplicates <- function(vec,find_cutoff=FALSE,cutoff_distance=0.06) {
wordcount <- vec %>%
tibble(txt=vec) %>%
dplyr::count(txt,sort=T)
words <- wordcount$txt
out <- sapply(seq_along(words)[-1],function(i) {
dist2 <- stringdist(words[i],words[1:i-1],method='jw',p=0.1)
best_fit <- which.min(dist2)
similarity2 <- min(dist2)
return(c(similarity2,best_fit))
}) %>%
t() %>%
as.data.frame() %>%
add_row(V1=1,V2=1,.before = 1) %>%
cbind(wordcount) %>%
dplyr::rename(distance=V1,best_fit=V2) %>%
mutate(replacement=txt[best_fit])
if(find_cutoff==TRUE) {return(out)}
dict <- out %>%
mutate(replacement=ifelse(distance<cutoff_distance,replacement,txt)) %>%
filter(replacement!=txt)
out_vec <- plyr::mapvalues(vec,from=dict$txt,to=dict$replacement)
return(out_vec)
}
#Blog Texts
test <- read.table("C:/Richard/R and Python/Datasets/blog.txt",sep="\n") %>%
unnest_tokens(word,V1) %>%
select(txt=word) %>%
tibble() %>%
count(txt,sort=T) %>%
mutate(best_fit=NA,similarity=1)
library(dplyr)
library(ggplot2)
library(stringdist)
library(tidytext)
library(tictoc)
#Blog Texts
test <- read.table("C:/Richard/R and Python/Datasets/blog.txt",sep="\n") %>%
unnest_tokens(word,V1) %>%
select(txt=word) %>%
tibble() %>%
count(txt,sort=T) %>%
mutate(best_fit=NA,similarity=1)
View(test)
#Blog Texts
test <- read.table("C:/Richard/R and Python/Datasets/blog.txt",sep="\n") %>%
unnest_tokens(word,V1) %>%
.$word
show <- remove_duplicates(test)
show <- remove_duplicates(test,find_cutoff=T)
View(show)
##ggmap
library(ggmap)
location <- c(-76,-56,-65,-15) #Chile
location <- c(-71.66,-33.078,-71.50,-33) #Valparaiso
location <- c(lon= -33.0500000,lat= -71.6000000)
location <- "Valparaiso, Valparaiso, Chile"
myMap <- get_map(location=location,source="google",maptype="terrain",crop=FALSE)
data <- read.csv("C:/Richard/R and Python/Datasets/Earthquakes Chile July 2019.csv")
library(ggplot2)
library(dplyr)
library(maps)
events_per_year <- data %>%
mutate(year=substr(time,1,4))
ggplot() +
borders(regions="Chile", colour="grey", fill=NA)+
geom_point(data=events_per_year,aes(x=longitude,y=latitude,col=mag,size=mag,alpha=mag))+
scale_size_continuous(breaks = c(5,6,7,8.8),range = c(0.5,3))+
xlim(c(-80,-65))+
ylim(c(-56,-17))+
facet_wrap(~year)
g <- ggplot() +
borders(regions="Chile", colour="grey", fill=NA)+
geom_point(data=events_per_year,aes(x=longitude,y=latitude,col=mag,size=mag,alpha=mag))+
scale_size_continuous(breaks = c(5,6,7,8.8),range = c(0.5,3))+
xlim(c(-80,-65))+
ylim(c(-56,-17))+
facet_wrap(~year)
plotly::ggplotly(g)
?plotly::ggplotly
plotly::ggplotly(g,tooltip=c("text","col"))
plotly::ggplotly(g,tooltip=c("text"))
events_per_year$place
g <- ggplot() +
borders(regions="Chile", colour="grey", fill=NA)+
geom_point(data=events_per_year,
aes(x=longitude,y=latitude,col=mag,
size=mag,alpha=mag,text=place))+
scale_size_continuous(breaks = c(5,6,7,8.8),range = c(0.5,3))+
xlim(c(-80,-65))+
ylim(c(-56,-17))+
facet_wrap(~year)
plotly::ggplotly(g,tooltip=c("text"))
install.packages("gtrendsR")
library(gtrendsR)
library(ggplot2)
ex <- gtrends(keyword=c("afp","covid"),geo="CL")
ex1 <- ex$interest_over_time
View(ex1)
ggplot(ex1,aes(x=date,y=hits,col=keyword))+geom_line()
str(ex1)
ex1 <- ex$interest_over_time %>%
mutate(hits=as.numeric(hits))
ex$interest_over_time$hits
ex1 <- ex$interest_over_time %>%
mutate(hits=as.numeric(hits))
ggplot(ex1,aes(x=date,y=hits,col=keyword))+geom_line()
?gtrends
ex <- gtrends(keyword=c("covid"),geo=c("CL","DE"))
ex1 <- ex$interest_over_time %>%
mutate(hits=as.numeric(hits))
ggplot(ex1,aes(x=date,y=hits,col=keyword))+geom_line()
View(ex1)
ggplot(ex1,aes(x=date,y=hits,col=geo))+geom_line()
ex <- gtrends(keyword=c("apruebo","rechazo"),geo=c("CL"))
ex1 <- ex$interest_over_time %>%
mutate(hits=as.numeric(hits))
ggplot(ex1,aes(x=date,y=hits,col=keyword))+geom_line()
ex <- gtrends(keyword=c("apruebo","rechazo"),geo=c("CL"),time="today 12-m")
ex1 <- ex$interest_over_time %>%
mutate(hits=as.numeric(hits))
ggplot(ex1,aes(x=date,y=hits,col=keyword))+geom_line()
